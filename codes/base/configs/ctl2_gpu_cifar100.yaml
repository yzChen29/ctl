# for cifar100

exp:
  name: "ctl_cifar100_test"   # folder name to save
  load_model_name: 'opt/app-root/src/cifar100_results/ctl_cifar100_test' # absolute path to load checkpoint

debug: False
memory_enable: True     # save memory after each training epoch
dataset_path: ''
df_name: ''

# architecture relatieve
use_joint_ce_loss: True
feature_mode: 'add_zero_only_ance' # ['full', 'add_zero_only_ance',  'add_zero_only_prev', 'add_zero_only_self']  # training stratefy
auto_retrain: False
bn_reset_running: False
bn_no_tracking: False
zero_init_residual: True

validation: 0  # Validation split (0. <= x <= 1.)

full_connect: False
use_connection: False

connect_fs: [512, 512]   # extracted feature for each layer
sample_rate: [0.3]       # sample rate r_1 ... r_n
epochs: 170
decouple:
  enable: True
  epochs: 50
  fullset: False
  lr: 0.05
  scheduling:
    - 15
    - 30
  lr_decay: 0.1
  weight_decay: 0.0005
  temperature: 5.0

# save/load checkpoint
retrain_from_task: 0  # 0 or n
save_mem: True
load_mem: False
show_noutput_detail: True
save_result_path: 'opt/app-root/src/cifar100_results/' # absolute path of a folder to save checkpoints
taxonomy: 'rtc' # rtc for Real Taxonomic Classifier
save_ckpt: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
save_before_decouple: False

# datasets relative
seed: 500
dataset: "cifar100"
trial: 5               # define the taxonomical tree, see /inclearn/datasets/datasets.py in detail
acc_k: 0

# check cgpu_using
show_detail: False    # show time consumsing or not
check_cgpu_info: False
check_cgpu_batch_period: 30

# training relative
lr: 0.1
batch_size: 128        # 256 or 64
num_workers: 4

# lr_decay
scheduling:
  - 100
  - 120

is_distributed: False


# Model Cfg
model: "incmodel"
model_cls: "hiernet"
model_pivot:
  arch: pivot
  dropout_rate: 0.0
convnet: 'resnet18'  # modified_resnet32, resnet18

train_head: 'softmax'
infer_head: 'softmax'
channel: 64
use_bias: False
last_relu: False

der: True
use_aux_cls: True
aux_n+1: True

distillation: "none"
temperature: 2

reuse_oldfc: True
weight_normalization: False
val_per_n_epoch: -1 # Validation Per N epoch. -1 means the function is off.

# Optimization; Training related
task_max: 10
lr_min: 0.00005
weight_decay: 0.0005
dynamic_weight_decay: False
scheduler: 'multistep'
lr_decay: 0.1
optimizer: "sgd"
resampling: False
warmup: False
warmup_epochs: 10

train_save_option:
  acc_details: True
  acc_aux_details: True
  preds_details: True
  preds_aux_details: True

postprocessor:
  enable: False
  type: 'bic'  #'bic', 'wa'
  epochs: 1
  batch_size: 128
  lr: 0.1
  scheduling:
    - 60
    - 90
    - 120
  lr_decay_factor: 0.1
  weight_decay: 0.0005

pretrain:
  epochs: 200
  lr: 0.1
  scheduling:
    - 60
    - 120
    - 160
  lr_decay: 0.1
  weight_decay: 0.0005

increment: 10

# validation: 0.1  # Validation split (0. <= x <= 1.)
random_classes: False  # Randomize classes order of increment
start_class: 0  # number of tasks for the first step, start from 0.
start_task: 0
max_task:  # Cap the number of task

# Memory
coreset_strategy: "iCaRL"  # iCaRL, random
mem_size_mode: "uniform_fixed_total_mem"  # uniform_fixed_per_cls, uniform_fixed_total_mem
memory_size: 2000  # Max number of storable examplars
fixed_memory_per_cls: 20  # the fixed number of exemplars per cls

# Misc
device_auto_detect: True  # If True, use GPU whenever possible
device: 0 # GPU index to use, for cpu use -1
#seed: 500
overwrite_prevention: False